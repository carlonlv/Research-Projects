%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[sigplan,10pt,review,anonymous]{acmart}
\acmSubmissionID{<202>}
\renewcommand\footnotetextcopyrightpermission[1]{}
\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan]{acmart}\settopmatter{}

%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmConference[PL'18]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2018}{New York, NY, USA}
\acmYear{2018}
\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{bbm}

\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}


\begin{document}

%% Title information
\title[Short Title]{PDSF: Harvesting Spare Cycles with QoS Guarantees for
Large-scale DataCenters}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{Jialun Lyu}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
Resources in data centers are often overprovisioned to keep latencies for
user-facing interactive services low, even in the case of load spikes or
server failures. One way to reduce the wastage of resources due to
overprovisioning is to use idle cycles on servers to run latency-insensitive
batch workloads. However, the co-location of batch workloads with
latency-sensitive services is challenging: If the utilization of the service
spikes while the server is simultaneously running a batch job, the service’s
latency might degrade, and the batch job might need to be killed and restarted
elsewhere. In this paper, we present PDSF (Probabilistic Dual-system Scheduler
Framework), a framework for schedulers that makes placement recommendations
that maximize utility of idle resources while maintaining an
administrator-specified limit on the tolerable number of resource violations
(events where a batch job is killed because it interferes with the resource
requirements of the co-located service). PDSF is based on practical,
light-weight statistical models, requires only a small amount of historical
information, and can easily be integrated into standard schedulers, such as
the Yarn scheduler. We show in simulations based on actual workload traces
that PDSF has the potential to utilize up to $40\%$ of the idle cycles in a
data center while limiting resource violations (where a background task is
killed due to a load spike) to less than $1\%$. We also discuss other use
cases of PDSF’s methods, for example, for workload autoscaling (as in Google’s
Autopilot).
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{keyword1, keyword2, keyword3}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

For a data center, jobs can be divided into two categories; One category
consists of jobs that have high priority, long runtimes, typically high latency
sensitivity, or, in general, require the highest availability, such as
user-facing service jobs, daemon jobs providing storage and networking
primitives, or monitoring jobs supervising the infrastructure of the entire
cluster. The other category consists of jobs that have low priority, short
runtimes, and in general, low cost to be killed or restarted (e.g., best-effort
jobs in Google Cloud Platform \cite{43438}). This paper references the former
category of jobs as foreground jobs and the latter category of jobs as
background jobs.

In a modern data center architecture with a state-of-the-art cluster manager,
such as Google's Borg system \cite{43438,10.1145/3342195.3387517}, foreground
jobs can often obtain resources by preempting or killing background jobs on the
same machines, even if the background jobs pass quota-checking during admission
protocol \cite{43438}. The cost of killing background jobs can be measured in
terms of wasting allocated computational resources by time and opportunity costs
of scheduling background jobs with smaller quota, or shorter runtimes, and
preempting jobs resulting in possibly delayed services and extra burdens on
scheduler and overheads. On the one hand, knowledge of future resource usage for
foreground jobs can result in more robust quota-checking for background jobs
scheduled on the same machines. On the other hand, exploiting background job
runtime knowledge also leads to adaptively optimized scheduler decisions from
various potential outcomes \cite{10.1145/3190508.3190515}.

The architecture of PDSF is also divided into two subsystems following
the two categories of jobs. A per-machine monitoring and a predicting system, which we refer to as \textit{Foreground PDSF},
based on the machine's information and active foreground jobs on that machine,
is used to make predictions on the available resources on that machine for some
given time windows into the future. A per-cluster clustering and scheduling
system, referred to as \textit{Background PDSF}, predicts the runtimes of submitted based on the jobs' features and assigns them to the optimal choice of machines according to the information of predicted available resources provided by the former subsystem. In the following
context, we use \textit{jobs} synonymously with \textit{tasks} as the smallest
unit of requests that need to fullied by assigning resources. 
\section{Background and Related Work}

Text of paper \ldots

\section{Predicting Foreground Jobs}
\label{sec:section3}

This section describes how PDSF predicts the available CPU resources for
a pre-specified time $t$ on a machine, given the history of CPU usage for that
machine. Such predictions can be passed to the scheduling component of a cluster
manager to decide whether a job for a certain amount of requested CPU resource
should be scheduled on this machine. Instead of point predictions of precisely
what the maximum CPU usage will be for the next time window $t$, PDSF
uses prediction intervals to find a safe margin for the point estimate of the
maximum CPU usage. 

There are at least two advantages of using interval predictions; One of them in
an obvious way is that the penalty for overestimating the available CPU
resources is higher than underestimating the available CPU resources. Irregular
behaviors, jumps, and outliers are frequent in some maximum traces in Microsoft
Azure Dataset, and they pose as the main threat of the failures of scheduled
background jobs. By using interval predictions, PDSF accounts for the
possibility of such irregularities happening at any time and hence increases the
chance of survival for the scheduled background jobs. Another advantage is that
a single parameter can control the width of prediction intervals; the cluster
manager can use such a parameter to tune how much it wants PDSF to be
conservative. The application to such a parameter can depend on the
functionality of clusters and the scheduler's historical performance. Based on
the survival rate and priority of jobs scheduled on a machine, the scheduler can
determine to utilize less possible resources in exchange for a higher survival
rate of jobs or the other way around. Such tradeoff will be elaborated further
in \cref{sec:section3.2}. 

\subsection{Foreground Model Selections}
\label{sec:section3.1}

In this section, we describe the mechanism of the foreground subsystem of Place
Holder, also provides a few candidate statistical models that are advantageous
over one another in different scenarios. 

Suppose that there is a a machine $m$ running a few foreground jobs (or tasks),
including the monitoring jobs that record the CPU usage (in percentage) in a
fixed frequency of $f$. In some data centers, the maximum and average CPU usage
over $\frac{1}{f}$  are also recorded simultaneously. The combined maximum or
average CPU usage of those jobs as a time series is referred to as
\textit{trace} in the following context. At any specific time $t$, we want to
predict the CPU resource available for next $w$ time window under probability of
$1 - \alpha$ (assuming $w$ is proportional to $\frac{1}{f}$), where $\alpha$ is
referred as \textit{cut off probability} in the following context. We aim to
construct a $1 - 2\alpha$ prediction interval for the maximum CPU usage. The
available resource for machine $m$, can be computed by $100 - u_{t, w,
m}(\alpha)$, where $u_{t, w, m}(\alpha)$, as a function of $\alpha$, represents
the upper bound of prediction interval within time interval from $t$ to $t + w$.

The computed $100 - u_{t, w, m}(\alpha)$ be interpreted as follows; The
available resouce is at least $100 - u_{t, w, m}(\alpha)$ from time $t$ to $t +
w$ with probability of $1 - \alpha$ if adequate models are used. The parameter
$\alpha$ controls the wideness of the prediction interval and therefore the
amount of "guaranteed" CPU resource under certain probability. If $\alpha$ is
small, then more conservative predictions are made, and smaller or fewer
background jobs can be scheduled on that machine. Since CPU resource is measured
in percentage, the time series of maximum CPU resource is a constrained time
series between $0$ and $100$. If $u_{t, w, m} \geq 100$ under a certain $\alpha$
with some models, then the "guaranteed" CPU resource within time interval $t$ to
$t + w$ is $0$, that is, the foreground predictor does not advise to schedule any
background jobs on this machine at this moment under the constrained probability
of $1 - \alpha$.

\subsubsection{Autoregressive models}

The simplest representative of Autoregressive models is AR(1) model with only
one lag, which means the current value of the time series depends on the
previous value plus some random noise. Before fitting the time series models
with the dataset, PDSF first aggregates the CPU resource values by
taking the maximum for each $w$ time window for all historical data, denoted as
$s_w[t]$. Then PDSF fits the AR(1) model by training the parameters
$\phi_0$, $\phi_1$ and $\sigma^2_{\epsilon}$ in the model:

\begin{equation}
  s_w[t] = \phi_0 + \phi_1 * s_w[t - 1] + \epsilon_t,
\end{equation}

where $\epsilon_t$ is a white noise process with mean $0$ and variance of
$\sigma^2_{\epsilon}$.

By applying AR(1) model on the foreground traces, we assume that the maximum CPU
usage of current window from $t$ to $t + w$ has an linear dependence of maximum
CPU usage of previous window from $t - w$ to $t$. It follows that we can
construct the upper bound of prediction interval of $s_w[t + 1]$ as,

\begin{equation}
  u_{t, w}(\alpha) = \phi_0 + \phi_1 * s_w[t - 1] + q_Z(1 - \alpha) * \sigma_{\epsilon},
\end{equation}

where we have observed the value of $s_w[t]$ at time $t$ and $q_Z(1 - \alpha)$
is the quantile of standard normal distributon at probability $1 - \alpha$.

\textbf{Distribution of residuals}: Using MLE(Maximum Likelihood Estimation) for
parameter estimations in an AR1 model and constructing prediction interval with
normal quantiles rely on the assumption that the error term is normally
distributed. However, by examining the sample skewness of the residuals with
method of moments estimations \cite{BONDON20091761}, we find that the
distribution of residuals for a significant amount of foreground traces are
asymmetric and right skewed, due to frequent appearances of jumps and outliers.
This suggests skewed normal seems to be a better fit, which compared to normal
distribution, has one more parameter to be estimated. More generally, we can
avoid assumptions on residuals being any parametrized distribution at all. By
using a bootstrapping algorithm \cite{PAN20161} to collect more residuals in the
form of an empirical distribution, PDSF then computes the prediction
upper bound by point predictions and the sample quantiles of bootstrapped
residuals.

\textbf{Outlier detections and estimations.} As described earlier, outliers are
very common throughout a significant number of foreground traces. Too frequent
occurrences of outliers result in overestimated variance, biased and uncertain
estimations on other parameters, too conservative prediction intervals, and the
incorrect assumption of normal distribution of residuals, especially outliers in
those traces are mostly jumps that are much higher than the average observed
values. To avoid the influence of outliers, PDSF uses automatic
detection procedures \cite{10.2307/2290724} by assuming the type of outliers to
be one of Additive Outlier(AO), Innovational Outlier(IO), Temporary Change(TC)
or Level shift(LS), and then estimates and removes the effect of outliers.

\textbf{The overhead of AR(1) Model.} As shown above, an AR(1) model is a
straightforward model with only three parameters to be estimated and kept track
of. Furthermore, for a simple AR1 model, it is easy to derive the closed-form of
the parameters in terms of training data, either by minimizing the conditional
sum of squares of residuals or maximizing the joint likelihood of data (under
the assumption of normally distributed error). Therefore, by using vector
operations, the computational cost for training is $O(n)$, where $n$ is the
number of aggregated data points in the training set. When updating an AR1
model, it is possible to use an efficient and accurate online gradient descent
algorithm to update parameters, which has complexity proportional to the size of
new observations \cite{Liu2016OnlineAA}. Under normality assumption or
skew-normal assumption of innovations, constructing prediction intervals is
simple and constant cost. However, using bootstrapping strategies to form
empirical distribution to construct prediction intervals can be costly in
computational and memory resources, depending on the number of paths sampled
\cite{PAN20161}. 

\textbf{External Regressor.} In addition to the information to past windowed
maximum observations, we may also include the information from past windowed
average observations, which is denoted as $a_w[t]$. Then the model becomes:

\begin{equation}
  s_w[t] = \phi_0 + \phi_1 * s_w[t - 1] + \phi_2 * a_w[t - 1] + \epsilon_t.
\end{equation}

Comparing to the AR(1) model, adding windowed average information does not
significantly increase the complexity of the model. This model will be refered
to as \textit{AR1X} in the following context.

\textbf{Variantions of AR(1) model.} As described earlier, most windowed maximum
traces contain irregularities, such as sudden jumps, outliers and change of
behaviors, violating the stationarity assumptions for fitting an AR(1) model. We
find that differencing the windowed maximum traces would significantly
decrease the effect of those irregular behaviors, making the resulting time
series much suitable for AR(1) model. The resulting model then becomes,

\begin{equation}
  \Delta s_w[t] = \phi_0 + \phi_1 * \Delta s_w[t] + \epsilon_t,
\end{equation}

where $\Delta s_w[t] = s_w[t] - s_w[t - 1]$ represents the differenced trace.
It can be interpreted as using AR(1) to model the change in the maximum
observations instead of modeling the original maximum observations directly.

\textbf{General ARIMA(p,d,q) model.} Since AR(1) is a simple model with only
three parameters, it can not capture more complicated patterns that can be found
in some windowed maximum traces. It is possible that more complicated models in
ARIMA family can be better fitted for those traces or even some segments for
some traces. For an ARIMA(p ,d ,q) model with parameter number of $p + q + 1$,
it may be an overfit to the training set with unnecessarily large $p$ and $q$,
resulting in a poor generalization of the entire trace and high variability in
predictions. Therefore, manually selecting a suitable number of $p$ and $q$ can
be a daunting task for every machine or even multiple time segments on those
machines. A classical automated procedure for selecting $p$ and $q$ is to
exhaustively search a predefined range of $p$ and $q$, then select the orders by
minimizing AIC(Akaike Information Criterion), which maximized the joint
likelihood of observations (with normality assumption) under a specific ARIMA
structure while penalizing the total number of parameters in the model
\cite{10.2307/2346970}.

\subsubsection{Markov models}

Another way of finding the available CPU resources is to divide the windowed
maximum observations into non-overlapping states. Each state $s$ covers a
specific range of $lower(s)$ and $upper(s)$ with the union of the range of all
states equals to $0$ to $100$. The intuition for considering a Markov model for
this problem is to consider the observations in bins as oppose to their exact
values, and predict a range (in states) of the observation in the next time
window with only the range information of the observation in the current time
window. Markov model trains the conditional distribution of states from one
window past using historical data. Comparing to AR(1) models that predict the
future value based on the linear correlation structure between itself and the
current value, Markov models also consider the nonlinear correlation between the
future state and the current state.

A Markov(m) model represents that the Markov model contains $m$ states and a
total number of $m^2$ parameters in the transition matrix $T$. To train the
transition matrix, PDSF first converts the windowed maximum data into
states, maps the state of each current window to the state of its succeeding
window, and then finally normalizes the columns of the transition matrix so that
the probability from any state to all other states sum to $1$. To find the
prediction upper bound of $1 - 2\alpha$ prediction interval of $s_w[t + 1]$,
PDSF searches for the destination state $d$ from the bottom state number
$1$ such that the cumulative summation of probabilities from bottom state to
destination states passes $1 - \alpha$. Mathematically, PDSF aims to
find,

\begin{equation}
  \begin{aligned}
  \min \quad & d\\
  \textrm{s.t.} \quad & \sum_{i = 1}^{d} T[s, i] \geq 1 - \alpha,
  \end{aligned}
\end{equation}

where $s$ is the state corresponding to $s_w[t]$, the purpose of this
formulation is because the summation of probabilities is not necessarily exactly
equal to $1 - \alpha$ for an arbitrary choice of $\alpha$. Finally, the available
resource can be computed as $100 - upper(d)$. 

\textbf{Partitioning methods.} There are two ways where the maximum observations
are partitioned into states; Fixed partitioning from range $0$ to $100$ and
quantile partitioning according to the sample quantiles of the training
observations. It is common for some traces that some of the states are untrained
in fixed partitioning method, that is, no or few observations in the training
set fall into those states, especially when the number of states is large.
Untrained states result in undefined conditional distribution when testing data
falls into the untrained states, which can be avoided using quantile
partitioning. Untrained states shou not be considered a severe problem for a
well trained Markov model which has enough observations used in the training
step. It is rare for well trained Markov models to have observations
in testing batch to fall into one of the untrained states. A simple solution to
avoid testing observation to fall into the untrained states is to use
unconditional distribution with all the training data.

\textbf{Number of states.} The most important parameter in Markov model is
choosing the number of states, which controls the total number of states and the
granularity of the background jobs that can be scheduled. For example, if fixed
partitioning with $m$ states are used, the available resource can only be
computed in multiples of $\frac{100}{m}$. If the choice of state number is too
large, then the occurrences of untrained states would be frequent, which leads
to uninformed predictions for test data falling into those states. However, if
the choice of state number is too few, then the predictions made would be
indifferentiable for a small range of cut off probability $\alpha$.

\textbf{Overhead of Markov(m) model.} The Markov models need to store one
transition matrix, which contains the number of parameters that equals the
square of the number of states. To train the transition matrix, one needs to
loop through all the training data, identifying the starting state and the
destination state, and then add them to the corresponding entry of the
transition matrix. Hence, the computational cost of training Markov-like models
is $O(n)$, where $n$ is the number of data in training size. As for updating the
transition matrix, combining weighted versions of the transition matrix and
newly trained transition matrix with new data is straightforward and has a
computational cost proportional to the number of new training data with the
fixed partitioning method. For the quantile partitioning method, recomputing
quantiles of training set and retraining transition matrix with a fixed training
size would be a more feasible solution. Making predictions is simple for
Markov-like models, especially for one-step prediction, which has complexity
proportional to the number of states. For $h$-step prediction, the computational
complexity becomes $O(h * m^3)$, since matrix multiplications are required.

\subsubsection{Vector autoregressive models}

Since both the average traces and maximum traces of simultaneous time windows
are recorded for some data centers, the Vector autoregressive model, as a
typical model to study multivariate time series data, can be used to forecast on
both average and maximum traces. In our case of bivariate time series, the
assumption for using VAR(p) model is that not only past time windows of the
maximum traces are considered to have a linear correlation relationship with the
current window of the maximum traces. The current window of average traces is
also considered to be linearly correlated to the current window of maximum
traces. Therefore, past and current observations in both maximum and average
traces can be used to forecast the future of the maximum traces (and vice
versa). A simple yet popular choice of VAR model, VAR(1) can be expressed as
follows:

\begin{equation}
  \begin{aligned}
    s_w[t] = \phi_{10} + \phi_{11} * s_w[t - 1] + \phi_{12} * a_w[t - 1] + \epsilon_t\\
    a_w[t] = \phi_{20} + \phi_{21} * s_w[t - 1] + \phi_{22} * a_w[t - 1] + e_t
  \end{aligned}
\end{equation}

where $e_t$ and $a_t$ are white noise processes and considered to be only
correlated at the same time window $t$. The prediction upper bound can be
derivatived as follows,

\begin{equation}
  u_{t, w}(\alpha) = \phi_{10} + \phi_{11} * s_w[t - 1] + \phi_{12} * a_w[t - 1] + q_Z(1 - \alpha) * \sigma_{\epsilon}.
\end{equation}

Even though the structure of VAR(1) models is very similar to two ARX(1) models
with the other time series as an external regressor, the external regressor in
ARX(1) model is considered to be an observed value (i.e., not a random
variable), yet in VAR(1) model, $a_w[t - 1]$ is a random variable correlated to
$s_w[t]$. For higher orders of VAR models, adding one additional order results
in adding $4$ additional parameters to the overall model. Therefore, it requires
taking precautions when increasing order to avoid overfitting and high
variability in estimated parameters. 
  
\textbf{Overhead of VAR(1) Models}: For a bivariate time series VAR(1) model
described above, a vector of length two storing the intercept terms and two $2$
by $2$ matrices storing the parameters and variance-covariance are needed,
bringing the total number of parameters to $10$. For AR(1) models, it is easy to
derive the close form of ordinary least squares estimation. The computation of
parameters has the complexity of $O(n)$ with $n$ being the training size.
However, with more complicated VAR models, a gradient descent algorithm may be
needed to estimate the parameters efficiently.
    
\subsection{Design of Simulation Study}
\label{sec:section3.2}

To compare the efficacy amongst different choices of foreground models, we
conduct a series of experiments with Microsoft Azure open dataset as foreground
traces. We sample $100$ pairs of approximately monthly long maximum and average
traces with a sample rate of $5$ minutes, fit different foreground models on a
training set, and then conservatively tries to fit the largest possible
background jobs at every single time window $w$ of the testing set for each
maximum trace, that is, the size of the jobs to be scheduled is exactly $100 -
u_{t, w}(\alpha)$ where $u_{t, w}(\alpha)$ is the prediction upper bound
computed by the model at time $t$.

Two scores are designed to measure the outcomes of each scheduling decision as a
function of $u_{t, w}(\alpha)$ and actual observation $s_w[t]$ of the window $t$
to $t + w$:

\begin{equation}
    \mathbf{survival_{t, w}(\alpha)} =
    \begin{cases}
    0 & u_{t, w}(\alpha) \leq s_w[t] \\
    1 & u_{t, w}(\alpha) \geq s_w[t] \\
    \text{undefined} & u_{t, w}(\alpha) \geq 100
    \end{cases}
\end{equation}

\begin{equation}
    \mathbf{utility{t, w}(\alpha)} =
   \begin{cases} 
    \frac{100 - u_{t, w}(\alpha)}{100 - s_w[t]} & s_w[t] = 1  \\
    0 & s_w[t] = 0 \\
    \text{undefined} & s_w[t] = 100
   \end{cases}
\end{equation}

The overall performance is measured by the averaging the performance for all
predictions made on the testing set across all sampled traces. The
\textit{undefined} result for both survival rate and utility rate is safely
ignored when taking the average. Undefined survival rates only happen whenever
the scheduler refuses to schedule any jobs, and undefined utility rates only
happen whenever there is no resource to be utilized by any jobs. Both
circumstances should not be penalized nor encouraged by our scoring system. 

\textbf{Limit of Fixed Ranged Observations.} Even though the interpretation of
the prediction interval suggests that the scheduled jobs following the formula
$100 - u_{t,w}(\alpha)$ have $1 - \alpha$ probability of survival when using
adequate models. However, as we observe in our many experiments, the survival
rate designed above never reaches $1 - \alpha$. Ideally, the construction of
prediction intervals does not have a limit on the range of the observations. The
upper bound of prediction intervals commonly exceeds $100$ for volatile traces.
However, it is not realistic to assign a negative CPU resouce to a job or any
resouce smaller than one CPU core. In these cases, the scheduler does not count
them as predictions made. Hence, by the design of our experiments, the survival
rate is lower than the level of the prediction interval.

\textbf{Tradeoff between survival and utilization.} The survival rate quantifies
how conservative the predictions are while the utility rate penalizes the
wideness of the prediction intervals' safety margin. Many of the designs in the
foreground system's mechanism rely on the tradeoff between survival score and
utility score. The most critical parameter by design that manages such
tradeoff is the \textit{cut off probability} $\alpha$ that inversely controls
the wideness of the prediction interval, which covers $1 - 2\alpha$ in
probability. 

Such tradeoff can be graphically represented by a line connecting a finite
number of points with each point representing a different cut off probability on
a 2D plane with survival rate and utility rate on the axes. Comparisons
against tradeoff curves under different parameter settings can identify which
parameter setting is a better option under a specific (or possibly all)
region(s). A better survival rate and a better utility rate shows a superior
modeling option. Therefore, whenever a tradeoff curve is on the top right of
another curve, it shows that the parameter setting (or model) corresponding to
the former curve is a better choice. 

\textbf{Aggregation vs extrapolation.} Suppose that we want to predict the
maximum value of $1$ hour window and the sampling rate of the data $\frac{1}{f}$
is $5$ minutes, we can aggregate $12$ points into one window, and make a
one-step prediction, or we can aggregate $6$ points into one window and then
make a two-step prediction, etc. In general, the number of aggregation steps and
extrapolation steps can be greater or equal to window size to predict $w$, but
the scheduler may take overly conservative action if the predicted size is much
greater than $w$. Taking more aggregation steps would mean more loss in
information during aggregation, and more extrapolation steps would result in
more unstable predictions and wider prediction intervals.

\textbf{Offline training vs online training.} Offline training refers to
training the model once and uses the model for predictions for the entire trace,
and Online training refers to training the model dynamically either at a fixed
amount of time (referred as \textit{Fixed Point training}) or when the scheduler
detects bad performances using the current model (referred as \textit{Dynamic
training}). Even though online training is more computationally expensive than
offline training depending on the retrain frequency, some of the models proposed
in \cref{sec:section3.1}  have a light and straightforward update algorithm,
which costs proportionally to the size of newly observed data. The reason for
developing an online training strategy over simple offline training is because
of the easily observed change of behaviours in some traces. The change of
behaviours includes temporary or permanent level shifts, change of variations,
or change of occurrences in outliers, and they all require new models to be
fitted at some frequency.

\textbf{Adjustment policy.} When a trace has a sudden rise in CPU usage, an
outdated foreground model trained based on previous observations tends to
overestimate the available resource and results in a segment of conservative
failures when trying to schedule jobs. To prevent this scenario from happening,
we can set a higher retrain frequency for the foreground model, but if such
behavior is temporary, the model will underestimate the available resource until
a retrain signal is received. Adjustment policy reduces consecutive failures in
such cases by refraining the model from scheduling anything when such an irregular
rise in CPU usage occurs until it goes back to normal. It can be used as a
complement of an infrequent setting of retrain frequency. Adjustment policy
essentially is a switch that automatically turns on when conservative failures
are detected, and turns off when such behavior passes and consecutive successes
are detected. However, adjustment policy as part of the scheduler decision while
being conservative and preventing the scheduling of jobs forfeits the actual
available CPU resources unless there is no resource to be utilized. The
utility when the adjustment switch is ON is always $0$. 

Two integer parameter, called \textbf{React Speed}, controls the behavior of
adjustment policy. The first parameter corresponds to the number of consecutive
failures to activate the adjustment switch, and the second parameter corresponds
to the number of consecutive sucesses to deactivate the adjustment switch. In a
scenario of having survival scores of $0,1,0$, a setting of react speed of
$1,1$, results in the worse outcome of $0$ sucesses out of $2$ predictions, as
oppose to $\frac{1}{3}$ of survival rate before using adjustment policy, or a
more careful setting of $2,1$ react speed. The example shows that in some cases,
not carefully chosen parameters for adjustment policy would result in a lower
overall survival rate and utility rate. 

\textbf{Granularity.} Since the allocation of CPU resources is achieved by
assigning CPU cores, granularity is evaluated by dividing $100$ percent by the
total number of cores of the machine representing the percentage of the total
CPU resource assigned to each core. Each machine can only assign resources as
multiples of granularity. In our simulation, a setting of granularity equals to
$0$ represents that granularity is not considered, and CPU resource can be any
real number between $0$ and $100$.

\subsection{Comparisons Against Autopilot}

The Autopilot suggested by Google \cite{10.1145/3342195.3387524} has a similar
purpose of recommending limits on CPU/memory resources of jobs by using
statistics over aggregated historical signals. Using the scoring metric
described in \cref{sec:section3.2}, we can compare the performance of
Autopilot's Moving Window recommender and our proposed foreground models on the
Microsoft Azure dataset 2019 \cite{cortez2017resource}. The moving window recommender aggregates the traces by
windows to histograms and then assign exponentially decaying weights by
$\frac{1}{2}$ per window. Then Autopilot uses \textit{jth-quantile} of
load-adjusted histograms to recommend the limit of resource usage of jobs on the
next window. The selection of $j$, equivalent to $1 - \alpha$, represents the
conservativeness of the recommended limit (which is a similar idea to prediction
upper bound) that can be used on foreground jobs. 

\textbf{Generate more granular traces.} Since the traces of the Microsoft
dataset have a sampling rate of $5$ minutes, we have to interpolate more
observations to have a histogram of sufficient original observations of CPU
usage per $5$-minute window by using maximum and average information for each
window.Here we demonstrate how we reconstruct the traces from a sample rate of
$5$ minutes($300$ seconds) to $1$ second. 

We want to find $x[t]$, the utility value of a trace at each second. We have
at each $t_i$ from $\{i = 0,1, ..., n; t_i - t_{i-1} = 300, t_0 = 0\}$, the
average utility $a[t]$ of each $5$ minutes window. The relationship between
$x[t]$ and $a[t]$ can be represented by 

\begin{equation}
  a[t_i] = \int_{t_{i-1}}^{t_i}\frac{1}{300}x[t]dt
\end{equation}

Consequently, we can compute the anti-derivative of $x[t]$, denoted as $F[t]$,
at sample points $\{t_0 = 0,t_1,...,t_n\}$, by 

\begin{equation}
  F[t_i] = \sum_{j=1}^{i} 300 * a[t_j] \approx \int_{0}^{t_i}x[t]dt
\end{equation}

Now, we provide three methods to interpolate samples for function $F[t]$:
\begin{itemize}
    \item \textbf{Version 1} uses a cubic interpolation spline function and
    sampling noises to match exact max observations and approximate (by Weak Law
    of Large Numbers) average observations \cite{911175}.
    \item \textbf{Version 2} uses bandlimited interpolation based on Shannon's
    sampling theorem and window-wise scaling to match exact max observations.
    \item \textbf{Version 3} uses a cubic interpolation spline and constrained
    sampling noise to match exact max and average observations.
\end{itemize}

Either way, we are able to obtain the values of $F$ at a finer granularity,
i.e., at time $k_i$ in $\{i = 0, 1, ..., m; k_i - k_{i-1} = 1, k_0 = 0\}$.
Approximating the derivative of $F[t]$ by taking numerical differences, we can
the reconstructed trace for $x[t]$ with $x[k_i] = F[k_i] - F(k_{i-1})$.

\textbf{Choice of half life.} Half life is a parameter in Autopilot representing
the time (in windows), after which the weight drops to $\frac{1}{2}$.
Equivalently, as $t$ goes one window to the past, the weight drops by
${\frac{1}{2}}^{\tau}$, where $\tau$ represents half life. The higher the
parameter is, the model puts more weight on previous histograms and therefore is
more long-memory. Ideally, the decay of weight can go to infinitely small,
requiring the scheduler to keep track of infinitely many past histograms. In a
more practical setup, a cut off weight is used to determine after which value
the weight will be regarded as zero, which means the scheduler only needs to
keep track of finitely many past histograms. The values of cut off weight and
half life determine the memory required for the Autopilot model.

\textbf{Number of breaks.} Since the limit is computed based on the quantile of
weighted histograms of past observations, the number of breaks controls the
partitioning of the histograms. Ideally, for different histograms of different
ranges, shapes, and variations, we develope a reasonable way of partitioning,
making sure there are enough observations in each bin to guaranteed the
histogram contains precise information of the empirical distribution. However,
the model keeps track of thousands of histograms of previous windows, each
containing $300$ observations within $5$-minute windows. It is not feasible to
manually examine each histogram to find out the best partitioning nor keep track
of the breaks as parameters for thousands of histograms. Therefore, we use the
same partitioning for all histograms to minimize the number of parameters, and a
simple setting of equal-sized bins in the range of $0$ and $100$.

\textbf{Overhead of Autopilot.} The memory overhead for Autopilot's Moving
Window recommender can be considered in two parts: The first part being the need
to record more fine-grained traces, which depending on the observations needed
for each histogram, could need hundreds of times more than using other proposed
models. The second part is that the total number of parameters needed to keep
track of is \textit{number of breaks} times \textit{number of histograms}. With
a choice of $12$ hours for the half life and \textit{cut off weight} of $0.01$,
$956$ histograms are computed and then kept in memory, bringing the total number
of parameters to $9560$ if, for each histogram, $10$ bins are used. The process
of putting past observations into bins has complexity linear to the number of
histograms times the sample rate in each window. A load-balanced and then
weighted histogram is constructed by the complexity linear to the number of
histograms used for predictions. In an overall sense, Autopilot's Moving Window
recommender requires much more memory and computational resource in use
comparing to parametric models suggested in \cref{sec:section3.1}.

\subsection{Comparisons Against Neural Network Models}

To have a general comparison of the efficacy of our proposed models in
\cref{sec:section3.1} with modern machine learning models, we select a feed
forward neural network as a simple yet popular representative for time series
forecasts. We use a simple FNN that contains one hidden layer and lagged series
of $s_w[t]$ as input. The number of neurons in the hidden layer is selected by
half of the input nodes plus $1$. Like General ARIMA models, the number of
parameters can also be selected by minimizing AIC, which maximizes the maximum
likelihood for observations and penalizes the total number of parameters.

\textbf{Overhead of NN Models}: The number of parameters of FNN is proportional
to the maximum lags of the input series. Training an FNN model requires a
backpropagation algorithm compsed by randomizing the parameters, computing the
loss by a feed forward algorithm and backpropagating to compute the gradient of
those parameters. This whole process is repeated multiple times until the
stopping criterion is reached \cite{5.4.374}. Comparing to AR(1) and VAR(1)
models that have closed form for their parameters, training a neural network is
a computationally expensive progress. Therefore, it is not feasible to
frequently update the trained FNN model for a temporary change of job behaviors.
While making point prediction for a FNN model is fast and straightforward,
constructing a prediction interval can be costly in memory and time consuming
since it requires bootstrapping similarly as AR1 models with empirical
distribution as described in \cref{sec:section3.1}. 

\subsection{Predicting Maxes VS Predicting Averages}

For some machines running batch jobs or other types of jobs that tolerate CPU
throttling, predicting averages of windows instead of maximum values would be
more suitable because using average information allows for extra wiggle room on
jobs scheduled on the same machine without affecting the progress of the jobs in
the long run.

The average traces are much less volatile, more stationary comparing to maximum
traces, and therefore are easier to predict with simple models.

\section{Predicting Background Jobs}
\label{sec:section4}

Before the prediction interval estimation in the foreground part, PDSF
estimates the future values' distribution either based on normality assumptions
or empirical distribution of the residuals. Similarly, PDSF evaluates
the background jobs' runtime distribution through a histogram of historical
observations. 

As suggested by Park et al. (2018) \cite{10.1145/3190508.3190515}, using
estimates of job runtime distribution enables the scheduler to make
better-informed decisions than point estimations. To generate the runtime
estimation of a new job, 3Sigma \cite{10.1145/3190508.3190515} first associates
the job with a set of features. Then 3Sigma finds the historical runtime of jobs
sharing the same features for each set of features. Different sets of features
generate different candidate histogram for distribution estimation of the
background job's runtime. 3Sigma selects one candidate distribution by comparing
each distribution's ability to make accurate point estimates on different
statistics designated for different situations.

PDSF utilizes the attributes of received background jobs in a slightly
different way. PDSF first clusters the historical records of naturally
finished jobs and then searches for the group of jobs with similar features
(e.g., requested resources, priority, hashed user name, and hashed job name) as
the job to predict. Based on runtime observations in the corresponding cluster,
PDSF constructs and returns a histogram of runtime. In this section, we
propose a few clustering models or algorithm and comment on their feasibility.
The performance of on different choice of models will be elaborated in
\cref{sec:section5}. 

\subsection{Background Model Selections}
\label{sec:section4.1}

\subsubsection{Gaussian Mixture Model Method}
\label{sec:section4.1.1}

A naive choice for clustering the jobs is a GMM model, which does the clustering
without using the response variable. The GMM model treats the joint distribution of each job's features as
multivariate normal random variable and models the whole dataset as a mixture
of multivariate normal distributions. Unlike the tree-structured clustering
methods, which will be explained in the latter context, the GMM model requires
the total number of clusters as an input. To simply the task of finding optimal number of clusters, background PDSF chooses the cluster number by fitting a range of numbers and find the number of clusters by minimizing BIC (Bayesian Information Criterion) in that range.

\textbf{Overhead of GMM model.} Training the GMM model requires time complexity
of $O(NKD^3)$, and $O(NKD^2)$ if rank-one updates are applied to inversion
matrices \cite{10.1371/journal.pone.0139931}, where $N$ is the number of data in
training, $K$ is the number of cluster chosen and $D$ is the dimension of the
dataset. Since PDSF uses only several attributes (e.g., requested
resources, scheduling class, priority) of accepted jobs to train GMM models, the
poor scalability of GMM models with high dimensional data is not of concern for
its feasibility in our case. A fast algorithm can be implemented for updating
the parameters in each Gaussian components under a feasible computational cost,
as provided by \cite{10.1371/journal.pone.0139931}.

\subsubsection{ANOVA tree models}
\label{sec:section4.1.2}

A classical decision tree, such as CART (Classification and Regression Tree)
\cite{Breimanbook}, follows two steps; grow an overly large tree using forward
selections and then prune or bag the tree to reduce complexity. At each step of
splitting, the tree finds the best node to split using impurity measures, such
as the Gini index or sum of squared errors, to minimize the impurity in each
node until each node contains the number of observations less than a
prespecified threshold. 

One of the tree-like structured models proposed in PDSF is to use the
total sum of squares for each node subtract by the added sum of squares for left
and right children when choosing the node to split (referred to as ANOVA tree in
the latter context). The minimal choice of such difference of the sum of squares
is to maximize the between-groups variability and minimize the within-groups
variability. This splitting criterion has similar effect of using F-statistics
as suggested by \cite{10.2307/24306157}. The resulting nodes have centralized
empirical distributions with significantly different means.

\textbf{Overhead of ANOVA tree models.} To manufacture an ANOVA tree based on a
large training set is costly when determining which variable to split and the
optimal boundary for binary outcomes. Even though calculating each statistics
when splitting is linear cost with respect to the number of observations under
each node, the total number of interation steps depends on the scale of the
dataset and termination condition. However, the computational speed of updating
the leaves of the tree is light and equivalent to training a smaller sized
subtree. In reality, it is feasible to train the tree model with an adequate
amount of data, use a stream updating algorithm \cite{756006.1756034} to update
the tree's structure to become robust. Moreover, a termination condition can be
prespecified to guarantee a minimum number of the dataset in each leaf to ensure
the histogram is not under fitted. The setting of the minial observations in
each leaf can also be used to tune the complexity of the tree and therefore
reduce the memory overhead. Similarly, the histograms at each of the leaves can
also be updated using a streaming-data algorithm \cite{756006.1756034}. 

\subsubsection{Survival tree models}
\label{sec:section4.1.3}

An alternative formulation of CART used for survival analysis in biology to
evaluate the life expectancy of biological organisms or the durability of
electronic parts in mechanical systems is well suited to model submitted jobs'
duration. Analogous to the ANOVA tree described above, the splitting criterion
for survival tree is selected to minimize the deviance between the mother node
and the sum of two child nodes as proposed in \cite{LeBlanc}. As opposed to the
ANOVA tree, which does not rely on any assumption of the data, the calculation of node's deviance relies on exponential distribution assumption on job's duration for survival tree. The duration within the same node of the tree have the same rate parameter. Intuitively, the model chooses the node with largest
deviance and splits the node by a single
point resulting in the greatest reduction in the deviance. The overall model is robust for non-exponentially
distributed data since histograms are used to approximate the observations'
empirical distribution instead of relying on the parametric exponential
distribution within each node. The overhead of survival tree model is similar to ANOVA tree
model descibed in \cref{sec:section4.1.2}.

\subsection{The Use of Discrete Features}
\label{sec:section4.2}

For some Data Centers, the hashed information of user names or jobs names are also recorded whenever a job is accepted. Such discrete information can be used as features for clustering algorithms under the intuition that variation of job's runtime within may be small. However, such modeling scheme suffers from poor scalability. 

For a clustering algorithm, discrete information is considered to be a factor with multiple levels. For example, each user name is considered to be a separate level different from other user names. When clustering algorithm is applied, other featured are used within each unique user name to create smaller clusters. For most users, the clustering algorithm hardly has enough observations to make use of other non-discrete information to make meaningful sub-clusters. The only sensible solution is to discard other information and builds histograms based on only one or two discrete features. Even so, one histogram per user or one histogram per program could potentially be a huge memory overhead for a large-scale data center. Moreover, some new users do not have enough observation to build a well-informing histogram. A clustering algorithm without relying on user names is still needed for these users. Histograms built for infrequent users may also poorly reflect the most recent behavior of the user, since observations are collection throughout a long time span. Finding jobs with similar features for their most recent behaviors is a better choice for predicting their new jobs' duration. Therefore, using discrete information is not efficient since clustering algorithm is needed to accommodate some special cases. 

\section{Combining Foreground PDFS and Background PDFS}
\label{sec:section5}

As a summary of \cref{sec:section3} and \cref{sec:section4}, Foreground PDSF requires a time window $w$ as input to predict the available resource over next window on a specific machine $m$. Background PDSF outputs a distribution of runtime for a submitted job $j$. In this section, we describe how a scheduler under PDSF combines the prediction result to make well-informed scheduling decisions.

\subsection{Scheduling Algorithm}
\label{sec:section5.1}

The algorithm for placing each job to one of a collection of machines at a specific time can be executed in the following steps:

\begin{enumerate}[Step 1]
    \item Background PDSF finds the cluster with closest features of the input job and returns a probability vector of the the job's runtime in each bin of the histogram in the background model.
    \item For each machine $m$, Foreground PDSF finds the prediction of available CPU cycles on that machine for the next time $w_i$ window, which represents the runtime value for bin $b_i$. The output in this step is a vector of available resources in different time windows for each machine.
    \item PDSF computes the estimated probability of SLA violation for a job-machine pair at time $t$.
    \item PDSF computes the expected utility for the scheduling of the job to machine $m$ by a utility function of the probability vector of the job's runtime, the vector of available resource in different time windows and the requested CPU. The expected utility is a non-negative number. However, if the estimated probability of SLA violation exceeds a prespecified threshold $\tau$, a special value of $-\infty$ is assigned to the utility of the job-machine pair.
    \item PDSF finds the machine that maximizes the utility for the job. If the expected utility for all machines is $-\infty$, it indicates that all machines can not accommodate the size and length of the job temporarily. The job is delayed until the next scheduling cycle.
\end{enumerate}

\textbf{Discretizing job's runtime}. To match the output of background models and the input of foreground models, PDSF discretizes the runtime of the jobs into bins of time windows. For each bin $b_i$ chosen, a corresponding foreground model is used to predict time windows $w_i$, where $w_i$ equals the right endpoint of $b_i$ for conservative estimations. Therefore, the bins needs to be chosen after the training step of Background PDSF and before the training step of Foreground PDSF to ensure the guaranteed matching in testing step.

Two aspects need to be considered when considering the choice of bins; Since the distributions of background jobs are estimated by histograms, the choice of bins needs to be fine-grained enough to have a decent estimation of the empirical distribution. However, for each bin added in the histograms, one more model needs to be fitted and kept track of in Foreground PDSF, hence increasing the overhead of the overall system. The selection of the bins can be preliminarily based on the unconditional distribution of runtime history for all finished jobs. An automatic procedure for robust selections as \cite{Freedman1981} can be deployed after each update for background models in Background PDSF. A request for adding or removing foreground models must be notified to Foreground PDSF whenever an update on partitioning of any histograms occurs. A constraint of the maximum number of bins can be specified for overhead control depending the type of foreground models chosen and the configuration of the cluster.

\textbf{Queuing jobs.} The scheduler of PDSF operates on a fixed period. At each cycle, the scheduler needs to decide which job to be scheduled first. Such decision is crucial if the jobs arrived are overwhelming and competing each other for resources on the machines. PDSF allows for a certain degree of freedom for ordering the queue of jobs. It is sensible to schedule jobs with higher priorities first and then the jobs with earlier deadlines. It is recommended to consider the jobs with highest requirement for CPU resources to be scheduled first for a higher schedule rate, since smaller jobs (in terms of requested CPU) are easier to be scheduled with smaller CPU fragments.

In reality, where the combinations of jobs and machines are overwhelming for a single CPU core to handle, decisions making can be parallelized. PDSF can split the jobs and machines randomly into multiple batches and make locally optimal decisions on considering the machines in the same batch as jobs for each batch. 

\textbf{Probability of SLA violation.} For a potential scheduling decision pair of a job and a machine, an SLA violation occurs resulting the scheduled job being killed due to insufficient amount of CPU resources at some time $t_k$. There are two uncertainties that contributes to a possible SLA violation; The actual CPU usage of foreground jobs exceeds the prediction upper bound at time $t_k$, and scheduler underestimate the runtime of the job based on the historical runtime of jobs with similar features. The former uncertainty in Foreground PDSF can be controlled by "cut off probability" as explained in \cref{sec:section3.1}. Similarly, we introduce a threshold for probability of SLA violations for scheduling a job $j$ on machine $m$ in time interval $(t, t + w)$ to control the latter uncertainty. The probability of SLA violation given CPU resource available on machine $m$ can be computed as:

\begin{equation}
    Pr(violation|A(m,t,w)) = \sum_{i = 1}^{n} Pr(R(j) = w_i) * \mathbbm{1}_{A(m,t,w) < R(j)},
\end{equation}

where $A(m,t,w)$ represents the available resource on $m$ from time $t$ to $t+w$, $R(j)$ is the requested CPU for job $j$ and $\mathbbm{1}$ is an indicator function.Since $A(m, t, w)$ is not accessible at time $t$, we use an estimate for it, that is, the "guaranteed" available resource $Q(m, t, w)$ with probability $1 - \alpha$ (where $\alpha$ is the cut off probability), which can be retrieved from the Foreground PDSF. If the estimated probability of SLA violation is between a certain threshold $\tau$, PDSF rejects assigning the job $j$ to the machine $m$ at time $t$.

\textbf{Expected utility function.} The expected utility of function quantifies the expected amount of CPU resources in the safe margin that can be utilized under the constraint that the probability of an SLA violation occurs is under a threshold $\tau$. At current time $t$, the expected utility for a machine-job pair is defined in a similar way as in \cite{10.1145/3190508.3190515}:

\begin{equation}
    E[U_m(j)] = \sum_{i = 1}^{n} U_m(requested(j), w_i) * Pr(w_i),
\end{equation}

where ($m$, $j$) represents the machine-job pair, $n$ is the total number of bins in the runtime histogram predicted by Background PDSF, $w_i$ is the upper bound of bin $b_i$ in the histogram, $Pr(w_i)$ is the value of the histogram. The utility function $U_m(request(j), runtime(j))$ as a function of required CPU resource and runtime of job $j$ on machine $m$ can be computed by a simple division of requested CPU resource by the predicted available CPU resource, representing the predicted percentage of utility over the next $runtime(j)$ time window. The utility for jobs that can not naturally finish based on predictions is defined to be $-\infty$, as they will not be scheduled on that machine. But there is some flexibility when defining utilities in such cases; If the scheduled jobs can tolerate some degree of throttling, then a negative penalty can be assigned to obtain a higher scheduling rate. A threshold on expected utility can be specified instead of using $\tau$ such that whenever the average utility is below it indicates scheduling such job is wasteful in terms of utility, and other jobs may have a better chance of harvesting the CPU cycles on that machine. The jobs that are denied to be scheduled to all machines are delayed to the next scheduling cycle.

\textbf{Handling SLA violations.} When the foreground workload spikes and interferes with scheduled background jobs on a machine, the scheduler must decide which scheduled jobs to be killed. In order to find the suitable jobs to kill to minimize the number of jobs being killed while also minimizing CPU utility lost, a two-step algorithm can be implemented as follows: 

\begin{enumerate}[Step 1]
    \item Order the current scheduled jobs by their requested CPU in descending order. Then by consecutively removing the jobs in order, we can find the minimum number of jobs, $S$, that needs to be killed in order to accommodate the spike in foreground CPU resource.
    \item Suppose that the amount of CPU resource needs to be freed is $C$, the scheduler consecutively select $S$ jobs to kill with minimum resource occupied of $C$ at current time, while minimizing loss as the result of killing such selected jobs. The loss for killing a scheduled job is defined to be $scheduled time * requested CPU$. A dynamic programming algorithm can be implemented by first selecting first $S$ elements in ordered jobs and compute their total loss and consecutively add one more job into the subset and compare the loss of new combinations and the minimum loss of sub-problem of size $S$. The algorithm terminates when the loss of new combinations of $S$ can not satisfy the constraint of freeing $C$ amount of CPU resources.
\end{enumerate}

The intuition for selecting jobs to kill is to find recent jobs that requires the largest amount of CPU resource. By minimizing loss, jobs that have been consuming a lot of resource for a long time have lower chance to be killed. Additional factors, such as priority, can also be added to the loss function. It is also noticeable that the runtime of the algorithm above is to compute loss of at most $nCk$ ($n$ choose $k$) subsets, which is approximately $O(n^k)$ where $n$ is the number of active jobs, $k$ is a positive integer representing the number of jobs that needs to be killed. A maximum number of searches can be specified to return the best selection of jobs in an allowed time constraint. Another alternative is a greedy algorithm is to simply order the jobs by their requested CPU in descending order and select jobs from top of the list to kill. This approach significantly reduce the complexity and does not have any impact on survival rate since the number of jobs being killed is exactly the same as dynamic programming algorithm described above, but the overall utility rate will be lower.

\subsection{Simulation Study}
\label{sec:section5.2}

To simulate the algorithm described in \cref{sec:section5.1}, we use Microsoft Azure open dataset 2017 \cite{cortez2017resource} as foreground traces and Google cluster usage trace 2011 \cite{43438} as background jobs to be scheduled. We aim to compare combinations of different foreground models of foreground PDSF, background models of background PDSF and two different distribution of jobs sampled in Google dataset. The design of the simulations is to create a busy scheduler with limited available resources, drastic change in foreground workload, and test the efficacy of the scheduling decisions with PDSF scheduler under such circumstances.

\textbf{Simulation settings.} We conduct experiments on two scenarios: $200,000$ tasks of unique jobs are sampled from naturally finished jobs in Google cluster, and scheduled on $50$ traces sampled from Microsoft cluster. Then, we use $15,000$ tasks of unique jobs with duration longer than $10$ minutes sampled from naturally finished jobs in Google cluster and schedule them on $15$ traces from Microsoft cluster to test the scheduler performance on longer tasks. The simulation lasts $1500$ minutes ($25$ hours) and the schedule cycle is once per $5$ minutes. The arrival time of background tasks are randomly assigned to the $300$ schedule cycles. According to the distribution of training set of foreground jobs, the breaks of output histograms of background PDSF are pre-specified as $0, 1-14, 18, 22, 26, 30, 50, 80, 205$ with unit of $5$ minutes. A total number of $21$ foreground models are pre-trained to predict available resources for the next time window equal to each bin of the histograms.

\textbf{Scheduler policy.} At each scheduling cycle, PDSF scheduler processes the jobs in descending order of their priority and their size of requested CPU. If an SLA violation occurs, and some scheduled jobs need to be killed on a machine, the scheduler uses the two-step dynamic programming algorithm described in \cref{sec:section5.1}.

\section{Conclusion}
text of conclusions...

%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}

%% Bibliography
\bibliography{reference}


%% Appendix
%\appendix
%\section{Appendix}

\end{document}
