---
title: "report"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("xlsx")
library("readxl")
library("ggplot2")
library("dplyr")
library("gridExtra")
library("pracma")
library("forecast")
library("mvtnorm")
library("arules")
library("dict")
library("cluster")
library("xlsx")
library("knitr")
library("kableExtra")

source("C://Users//carlo//Documents//GitHub//Research-Projects//ForegroundJobScheduler//rscript//helper_functions.R")
```


```{r, echo=FALSE}
read_runs <- function(rownum, parameter.df, model_name, sample_size, result_folder_path) {
  param_list <- parameter.df[rownum,]
  
  window_size <- param_list[1]
  prob_cut_off <- param_list[2]
  granularity <- param_list[3]
  
  filename <- NULL
  if (model_name == "AR1_logistic_lm") {
    bin_num <- param_list[4]
    filename <- paste("Overall Runs", model_name, sample_size, window_size, prob_cut_off, granularity, bin_num, ".csv")
  } else if (model_name == "AR1_state_based_logistic") {
    state_num <- param_list[4]
    filename <- paste("Overall Runs", model_name, sample_size, window_size, prob_cut_off, granularity, state_num, ".csv")
  } else {
    filename <- paste("Overall Runs", model_name, sample_size, window_size, prob_cut_off, granularity, ".csv")
  }
  
  runs <- read.csv(paste(result_folder_path, filename, sep=""))
  runs <- colSums(runs[, -1])
  return(runs)
}


build_2d_dataframe <- function(runs) {
  
  y <- colnames(runs)
  x <- rownames(runs)
  result <- expand.grid(y, x)
  colnames(result) <- c("y", "x")
  result$z <- NA
  for (i in 1:length(x)) {
    for (j in 1:length(y)) {
      result[length(y)*(i-1)+j, 3] <- runs[i,j]
    }
  }
  return(result)
}


convert_frequency_dataset <- function(dataset, new_freq, mode) {
  new_avg_cpu <- c()
  window_num <- NULL
  window_num <- floor(length(dataset) / new_freq)
  for (i in 1:window_num) {
    from <- (i - 1) * new_freq + 1
    to <- i * new_freq
    new_val <- NULL
    if (mode == 'max') {
      new_val <- max(dataset[from:to], na.rm = TRUE)
    } else {
      new_val <- mean(dataset[from:to], na.rm = TRUE)
    }
    new_avg_cpu <- c(new_avg_cpu, new_val)
  }
  return(new_avg_cpu)
}
```

```{r, echo=FALSE}
find_bin_obs <- function(avg, binsize) {
  return(floor(avg / binsize))
}


train_cond_var_model <- function(ts_num, train_set_max, train_set_avg, bin_num) {
  
  new_parsed_dat <- data.frame(matrix(nrow=nrow(train_set_avg), ncol=3))
  binsize <- 100 / bin_num
  bin <- as.numeric(sapply(train_set_avg[,ts_num], find_bin_obs, binsize))
  bin <- bin * binsize
  for (i in 1:nrow(train_set_avg)) {
    new_parsed_dat[i,] = c(train_set_avg[i, ts_num], train_set_max[i, ts_num], bin[i])
  }
  
  colnames(new_parsed_dat) <- c('avg', 'max', 'bin')
  selected_bins <- new_parsed_dat %>%
    group_by(bin) %>%
    count()
  
  selected_bins <- subset(selected_bins, selected_bins$n >= 3)$bin
  selected_bins <- new_parsed_dat$bin
  new_parsed_dat1 <- new_parsed_dat %>%
    filter(bin %in% selected_bins) %>%
    group_by(bin) %>% 
    summarise(sd=sqrt(var(max))) %>%
    filter(!is.na(sd))
  
  sd.lm <- NULL
  if (nrow(new_parsed_dat) >= 3) {
    sd.lm <- lm(sd~bin+I(bin^2), data = new_parsed_dat1)
  } else if (nrow(new_parsed_dat) == 2) {
    sd.lm <- lm(sd~bin, data = new_parsed_dat1)
  } else {
    sd.lm <- new_parsed_dat1$sd
  }
  return(list("lm"=sd.lm, "data"=new_parsed_dat1, "data2"=new_parsed_dat))
}
```


```{r, echo=FALSE}
make_fitted_model_plot <- function(dataset, lm) {
  predicted <- predict(lm, newdata=data.frame(bin=linspace(min(dataset$bin), max(dataset$bin), 100)))
  predicted_df <- data.frame(x=linspace(min(dataset$bin), max(dataset$bin), 100), y=predicted)
  plt <- ggplot() +
    geom_point(aes(x=bin, y=sd), dataset) +
    geom_line(aes(x=x, y=y), predicted_df) +
    ylab("Maxes") +
    xlab("Bin")
  return(plt)
}


make_qqnorm_plot <- function(dataset) {
  df <- dataset %>%
    group_by(bin) %>%
    count()
  binnum <- df$bin[df$n == max(df$n)]
  df <- dataset %>%
    filter(bin == binnum)
  plt <- ggplot(df, aes(sample = max)) +
    stat_qq() +
    stat_qq_line()
  return(plt)
}


make_hist <-  function(dataset) {
  breaks <- seq(0, 50, 10)
  dataset$hst <- NA
  for (i in 1:nrow(dataset)) {
    j = 1
    current_bin <- dataset[i, ]$bin
    while (j <= (length(breaks) - 1)) {
      if (current_bin <= breaks[j+1]) {
        break
      }
      j <- j + 1
    }
    dataset[i, ]$hst <- paste("From", breaks[j], "To", breaks[j+1])
  }
  plt <- ggplot(dataset, aes(x=max, color=hst, alpha=0.5)) + 
    geom_histogram(bins=20, fill="white", show.legend = FALSE) + 
    xlab("Maxes") +
    ylab("Frequency")
  return(plt)
}


plot_results <- function(model_results, sample_size, window_size, model_name="NULL") {
  model_results <- model_results %>% 
    filter(Sample.Size== sample_size & Window.Size == window_size)
  if (model_name == "ar_logistic_lm") {
    ggplot(model_results, aes(x=Survival.Rate, y=Avg.Cycle.Usage)) +
      geom_point(na.rm=TRUE, aes(color=factor(Model), shape=factor(Cpu_usage), alpha=factor(Granularity))) + 
      geom_vline(xintercept=0.99, linetype="dashed", color="red") + 
      ylab("Utilization") +
      xlab("Survival Rate") + 
      ggtitle(paste("Model Performance With Sample Size", sample_size, "and Window Size", window_size))
  } else {
    ggplot(model_results, aes(x=Survival.Rate, y=Avg.Cycle.Usage)) +
      geom_point(na.rm=TRUE, aes(color=factor(Model), shape=factor(StateNum), alpha=factor(Granularity))) + 
      scale_shape_manual(values=c(15, 16,17,18,21,22,23,24)) +
      geom_vline(xintercept=0.99, linetype="dashed", color="red") + 
      ylab("Utilization") +
      xlab("Survival Rate") + 
      ggtitle(paste("Model Performance With Sample Size", sample_size, "and Window Size", window_size))
  }
}


plot_conditional_hist <- function(dataset) {
  
  df <- dataset %>%
    group_by(bin) %>%
    count()
  binnum <- df$bin[df$n == max(df$n)]
  df <- dataset %>%
    filter(bin == binnum)
  
  plt <- ggplot(df, aes(x=max)) +
    geom_histogram(aes(y = ..density..), binwidth=density(df$max)$bw) +
    xlab("Maxes") +
    ggtitle(paste(binnum))
  return(plt)
}
```


## Overall Performance

```{r echo=FALSE}
sample_size <- 100
window_size <- 12

ar_data_path <- "C://Users//carlo//Documents//GitHub//Research-Projects//ForegroundJobScheduler//results//Nonoverlapping windows//summary dynamic (windows,granularity).xlsx"

ar_xlsx <- read.xlsx(ar_data_path, sheetIndex = 1)
plot_results(ar_xlsx, sample_size, window_size)

ar_data_path <- "C://Users//carlo//Documents//GitHub//Research-Projects//ForegroundJobScheduler//results//Nonoverlapping windows//summary (windows,granularity).xlsx"

ar_xlsx <- read.xlsx(ar_data_path, sheetIndex = 1)
plot_results(ar_xlsx, sample_size, window_size)
```

## Runs for Different Models

```{r echo=FALSE}
sample_size <- 100

result_folder <- "C://Users//carlo//Documents//GitHub//Research-Projects//ForegroundJobScheduler//results//Nonoverlapping windows//maxes//"

window_size <- c(12)
prob_cut_offs <- c(0.005, 0.01, 0.1, 0.5)
granularity <- c(10, 100/32, 100/128, 0)
num_of_bins <- c(1000, 500)

param <- expand.grid(window_size, prob_cut_offs, granularity, num_of_bins)
runs <- sapply(1:nrow(param), read_runs ,param, "AR1_logistic_lm", sample_size, result_folder)
rownames(runs) <- 1:20
colnames(runs) <- apply(param, 1, function(x) paste(x, collapse=" "))
runs.df <- build_2d_dataframe(runs)
ggplot(runs.df, aes(x=x, y=y, size=z, color=z)) + 
  geom_point(alpha=0.5) +
  xlab("length of runs") +
  ylab("Parameters") +
  ggtitle("Distribution of runs for AR1_logistic_lm")
``` 

```{r echo=FALSE}
window_size <- c(12)
prob_cut_offs <- c(0.005, 0.01, 0.02, 0.1)
granularity <- c(10, 100/32, 100/64, 100/128, 0)

param <- expand.grid(window_size, prob_cut_offs, granularity)
runs <- sapply(1:nrow(param), read_runs ,param, "AR1", sample_size, result_folder)
rownames(runs) <- 1:20
colnames(runs) <- apply(param, 1, function(x) paste(x, collapse=" "))
runs.df <- build_2d_dataframe(runs)
ggplot(runs.df, aes(x=x, y=y, size=z, color=z)) + 
  geom_point(alpha=0.5) +
  xlab("length of runs") +
  ylab("Parameters") +
  ggtitle("Distribution of runs for AR1")
```

```{r echo=FALSE}
window_size <- c(12)
prob_cut_offs <- c(0.005, 0.01, 0.02, 0.1)
granularity <- c(10, 100/32, 100/64, 100/128, 0)

param <- expand.grid(window_size, prob_cut_offs, granularity)
runs <- sapply(1:nrow(param), read_runs ,param, "VAR1", sample_size, result_folder)
rownames(runs) <- 1:20
colnames(runs) <- apply(param, 1, function(x) paste(x, collapse=" "))
runs.df <- build_2d_dataframe(runs)
ggplot(runs.df, aes(x=x, y=y, size=z, color=z)) + 
  geom_point(alpha=0.5) +
  xlab("length of runs") +
  ylab("Parameters") +
  ggtitle("Distribution of runs for VAR1")
```

## Conditional Variance

```{r, echo=FALSE}
bg_jobs_path = "C://Users//carlo//Documents//sample background jobs//"
bg_job_pool <- read.csv("C://Users//carlo//Documents//GitHub//Research-Projects//ForegroundJobScheduler//pythonscripts//list of sampled 100 background jobs.csv")[,1]
bg_job_pool <- sample(bg_job_pool, 9)
bg_job_pool <- sub(".pd", "", bg_job_pool)
train_set_avg <- matrix(nrow = 6000, ncol = 0)
train_set_max <- matrix(nrow = 6000, ncol = 0)
for (job_num in bg_job_pool) {
  bg_job <- read.csv(paste(bg_jobs_path, job_num, ".csv", sep = ""))
  train_set_avg <- cbind(train_set_avg, bg_job$avg_cpu[1:6000])
  train_set_max <- cbind(train_set_max, bg_job$max_cpu[1:6000])
}
rownames(train_set_avg) <- seq(1, nrow(train_set_avg) ,1)
rownames(train_set_max) <- seq(1, nrow(train_set_max) ,1)
colnames(train_set_avg) <- bg_job_pool
colnames(train_set_max) <- bg_job_pool
```


```{r, echo=FALSE}
cpu_required <- rep(0, ncol(train_set_max))
for (j in 1:ncol(train_set_max)) {
  cpu_required[j] <- as.numeric(quantile(train_set_max[,j], c(0.15, 0.5, 0.85), type = 4)[3])
}

window_size <- c(12)
prob_cut_off <- c(0.1)
granularity <- c(100/128)
bin_num <- c(1000)

new_trainset_max <- apply(train_set_max, 2, convert_frequency_dataset, new_freq=window_size, mode="max")
rownames(new_trainset_max) <- seq(1, 1 + window_size * (nrow(new_trainset_max) - 1), window_size)
colnames(new_trainset_max) <- colnames(train_set_max)

new_trainset_avg <- apply(train_set_avg, 2, convert_frequency_dataset, new_freq=window_size, mode="avg")
rownames(new_trainset_avg) <- seq(1, 1 + window_size * (nrow(new_trainset_avg) - 1), window_size)
colnames(new_trainset_avg) <- colnames(train_set_max)
```

```{r, echo=FALSE}
window_size <- c(12)
prob_cut_off <- c(0.1)
granularity <- c(100/128)
bin_num <- c(1000)

new_trainset_max <- apply(train_set_max, 2, convert_frequency_dataset, new_freq=window_size, mode="max")
rownames(new_trainset_max) <- seq(1, 1 + window_size * (nrow(new_trainset_max) - 1), window_size)
colnames(new_trainset_max) <- colnames(train_set_max)

new_trainset_avg <- apply(train_set_avg, 2, convert_frequency_dataset, new_freq=window_size, mode="avg")
rownames(new_trainset_avg) <- seq(1, 1 + window_size * (nrow(new_trainset_avg) - 1), window_size)
colnames(new_trainset_avg) <- colnames(train_set_max)

cond_var_models <- sapply(1:9, train_cond_var_model, new_trainset_max, new_trainset_avg, bin_num, simplify=FALSE)
```

```{r, echo=FALSE}
p <- list()
for (i in 1:9) {
  p[[i]] <- make_fitted_model_plot(cond_var_models[[i]]$data, cond_var_models[[i]]$lm)
}
do.call(grid.arrange, p)
```

```{r, echo=FALSE}
q <- list()
for (i in 1:9) {
  q[[i]] <- make_qqnorm_plot(cond_var_models[[i]]$data2)
}
do.call(grid.arrange, q)
```

```{r, echo=FALSE}
r <- list()
for (i in 1:9) {
  r[[i]] <- make_hist(cond_var_models[[i]]$data2)
}
do.call(grid.arrange, r)
```

```{r, echo=FALSE}
s <- list()
for (i in 1:9) {
  s[[i]] <- plot_conditional_hist(cond_var_models[[i]]$data2)
}
do.call(grid.arrange, s)
```

```{r, echo=FALSE}
generate_expected_conditional_var <- function(expected_avgs, mode, variance_model) {
  expected_var <- c()
  if (mode == "lm") {
    if (is.numeric(variance_model)) {
      expected_var <- rep(variance_model^2, length(expected_avgs))
    } else {
      expected_var <- predict(variance_model, newdata=data.frame("bin"=expected_avgs), type = "response")^2
    }
  }
  return(max(expected_var, 0))
}


train_ar1_model <- function(ts_num, train_dataset) {
  ts_model <- tryCatch({
    arima(x=train_dataset[, ts_num], order = c(1,0,0), include.mean = TRUE, method = "CSS-ML", optim.control = list(maxit=2000))
  }, error = function(cond) {
    return(arima(x=train_dataset[, ts_num], order = c(1,0,0), include.mean = TRUE, method = "ML", optim.control = list(maxit=2000)))
  })
  return(list("coeffs"=as.numeric(ts_model$coef[1]), "means"= as.numeric(ts_model$coef[2]), "vars"=ts_model$sigma2))
}


parser_for_logistic_model <- function(train_set_max, train_set_avg, cpu_required) {
  df <- data.frame("avg"=train_set_avg, "max"=train_set_max)
  df$survived <- ifelse(df$max <= (100 - cpu_required), 1, 0)
  return(df)
}


train_logistic_model <- function(ts_num, train_dataset_max, train_dataset_avg, cpu_required) {
  logistic_input <- parser_for_logistic_model(train_dataset_max[,ts_num], train_dataset_avg[,ts_num], cpu_required[ts_num])
  log.lm <- suppressWarnings(glm(survived~avg, data = logistic_input, family = "binomial", control=glm.control(maxit=50)))
  return(log.lm) 
}


do_prediction <- function(last_obs, phi, mean, variance) {
  # Construct mean
  mu <- last_obs * phi + (1 - phi) * mean
  # Construct Var-cov matrix
  var <- variance
  result <- list('mu' = mu, 'var'=var)
  return(result)
}


find_bin_obs <- function(avg, binsize) {
  return(floor(avg / binsize))
}


train_cond_var_model <- function(ts_num, train_set_max, train_set_avg, bin_num, method) {
  if (method == "lm") {
    
    new_parsed_dat <- data.frame(matrix(nrow=nrow(train_set_avg), ncol=3))
    binsize <- 100 / bin_num
    bin <- as.numeric(sapply(train_set_avg[,ts_num], find_bin_obs, binsize))
    bin <- bin * binsize
    for (i in 1:nrow(train_set_avg)) {
      new_parsed_dat[i,] = c(train_set_avg[i, ts_num], train_set_max[i, ts_num], bin[i])
    }
    
    colnames(new_parsed_dat) <- c('avg', 'max', 'bin')
    selected_bins <- new_parsed_dat %>%
      group_by(bin) %>%
      count()
    
    selected_bins <- subset(selected_bins, selected_bins$n >= 3)$bin
    selected_bins <- new_parsed_dat$bin
    new_parsed_dat <- new_parsed_dat %>%
      filter(bin %in% selected_bins) %>%
      group_by(bin) %>% 
      summarise(sd=sqrt(var(max))) %>%
      filter(!is.na(sd))
    
    sd.lm <- NULL
    if (nrow(new_parsed_dat) >= 3) {
      sd.lm <- lm(sd~bin+I(bin^2), data = new_parsed_dat)
    } else if (nrow(new_parsed_dat) == 2) {
      sd.lm <- lm(sd~bin, data = new_parsed_dat)
    } else {
      sd.lm <- new_parsed_dat$sd
    }
    return(sd.lm)
  } else {
    clustering_result <- list()
    avg_silhouette <- c()
    avg_silhouette[1] <- -Inf
    for (cluster_num in 2:10) {
      clustering_result[[cluster_num]] <- kmeans(train_set_avg[, ts_num], centers = cluster_num, iter.max = 20, nstart = 25)
      avg_silhouette[cluster_num] <- mean(silhouette(clustering_result[[cluster_num]]$cluster, dist = dist(train_set_avg[,ts_num]))[,3])
    }
    best_cluster_num <- which(avg_silhouette == max(avg_silhouette))
    best_cluster_result <- clustering_result[[best_cluster_num]]
    new_parsed_dat <- data.frame("cluster"=best_cluster_result$cluster, "max"=train_set_max)
    new_parsed_dat <- new_parsed_dat %>% 
      group_by(cluster) %>% 
      summarise(vars=var(max)) %>%
      filter(!is.na(vars))
    new_parsed_dat$cluster_mean <- best_cluster_result$centers[new_parsed_dat$cluster]
    return(new_parsed_dat)
  }
}


find_expected_max <- function(probability, variance, cpu_required, expected_avgs) {
  if (probability == 1) {
    return(100)
  } else if (probability == 0) {
    return(expected_avgs)
  } else {
    return(max((100 - cpu_required) - qnorm(p=(1-probability)) * sqrt(variance), expected_avgs))
  }
}


scheduling_model <- function(ts_num, test_dataset_max, test_dataset_avg, coeffs, means, vars, logistic_models, cond_var_models, cond.var, window_size, prob_cut_off, cpu_required, granularity, schedule_policy) {
  utilization <- c()
  survival <- c()
  
  seek_length <- window_size
  last_time_schedule <- nrow(test_dataset_max) - window_size + 1
  
  logistic_model <- logistic_models[[ts_num]]
  cond_var_model <- cond_var_models[[ts_num]]
  
  current_end <- window_size + 1
  update_policy <- ifelse(schedule_policy == "disjoint", window_size, 1)
  
  avgs <- c()
  vars <- c()
  probs <- c()
  maxes <- c()
  pi_ups <- c()
  
  while (current_end <= last_time_schedule) {
    last_obs <- convert_frequency_dataset(test_dataset_avg[(current_end-window_size):(current_end-1), ts_num], window_size, mode = 'avg')
    
    expected_avgs <- do_prediction(last_obs, coeffs[ts_num], means[ts_num], vars[ts_num])$mu
    
    expected_vars <- generate_expected_conditional_var(expected_avgs, mode = cond.var, variance_model = cond_var_model)
    
    prob <- 1 - predict(logistic_model, newdata = data.frame("avg"=expected_avgs), type = "response")
    
    expected_max <- find_expected_max(prob, expected_vars, cpu_required[ts_num], expected_avgs)
    
    pi_up <- compute_pi_up(mu=expected_max, varcov=as.matrix(expected_vars), predict_size=1, prob_cutoff=prob_cut_off, granularity=granularity)

    start_time <- current_end
    end_time <- current_end + seek_length - 1
    position_vec <- convert_frequency_dataset(test_dataset_max[start_time:end_time, ts_num], window_size, "max")
    evalulation <- find_evaluation(pi_up=pi_up, actual_obs=position_vec, granularity=granularity)
    utilization <- c(utilization, evalulation$usage)
    survival <- c(survival, evalulation$survival)
    
    if (schedule_policy == "dynamic") {
      if (!is.na(evalulation$survival) & evalulation$survival == 1) {
        update_policy <- window_size
        
      } else if (!is.na(evalulation$survival) & evalulation$survival == 0) {
        update_policy <- 1
      }
    }
    avgs <- c(avgs, expected_avgs)
    vars <- c(vars, expected_vars)
    probs <- c(probs, prob)
    maxes <- c(maxes, expected_max)
    pi_ups <- c(pi_ups, pi_up)
    
    
    current_end <- current_end + update_policy
  }
  
  overall_rate <- find_overall_evaluation(utilization, survival)
  return(list("utilization"=overall_rate$utilization_rate, "survival"=overall_rate$survival_rate))
}


ar_logistic_model <- function(dataset_avg, dataset_max, initial_train_size, prob_cut_off, update_freq, window_size, cpu_required, cond.var, granularity, bin_num=NULL) {

  if (granularity > 0) {
    cpu_required <- sapply(cpu_required, round_to_nearest, granularity, FALSE)
  }
  
  ts_names <- colnames(dataset_avg)
  
  result.df <- data.frame(matrix(nrow=length(ts_names), ncol=0))

  ## Lists
  logistic_models <- NULL
  cond_var_models <- NULL
  
  ## Split the dataset into training and testing sets
  train_dataset_max <- dataset_max[1:initial_train_size,]
  test_dataset_max <- dataset_max[(initial_train_size+1):nrow(dataset_max),]
  train_dataset_avg <- dataset_avg[1:initial_train_size,]
  test_dataset_avg <- dataset_avg[(initial_train_size+1):nrow(dataset_avg),]
  
  ## Convert Frequency for trainning set
  new_trainset_max <- apply(train_dataset_max, 2, convert_frequency_dataset, new_freq=window_size, mode="max")
  rownames(new_trainset_max) <- seq(1, 1 + window_size * (nrow(new_trainset_max) - 1), window_size)
  colnames(new_trainset_max) <- ts_names
  
  new_trainset_avg <- apply(train_dataset_avg, 2, convert_frequency_dataset, new_freq=window_size, mode="avg")
  rownames(new_trainset_avg) <- seq(1, 1 + window_size * (nrow(new_trainset_avg) - 1), window_size)
  colnames(new_trainset_avg) <- ts_names
  
  ## Training AR1 Model
  train_result <- sapply(1:length(ts_names), train_ar1_model, new_trainset_avg)
  coeffs <- unlist(train_result[1,])
  means <- unlist(train_result[2,])
  vars <- unlist(train_result[3,])
  
  ## Training Logistic Model
  logistic_models <- sapply(1:length(ts_names), train_logistic_model, new_trainset_max, new_trainset_avg, cpu_required, simplify=FALSE)
  
  ## Training Polynomial Regression Model
  cond_var_models <- sapply(1:length(ts_names), train_cond_var_model, new_trainset_max, new_trainset_avg, bin_num, cond.var, simplify=FALSE)
  
  ## Test Model
  result_model <- sapply(1:length(ts_names), scheduling_model, test_dataset_max, test_dataset_avg, coeffs, means, vars, logistic_models, cond_var_models, cond.var, window_size, prob_cut_off, cpu_required, granularity, schedule_policy, simplify=FALSE)
  
  for (ts_num in 1:length(ts_names)) {
    result.df <- rbind(result.df, c(result_model[[ts_num]]$utilization, result_model[[ts_num]]$survival))
  }
  
  rownames(result.df) <- ts_names
  colnames(result.df) <- c("avg_usage", "survival")
  
  tbl <- kable(result.df, format = 'pandoc', align = "c", caption = paste("Cut off Prob:", prob_cut_off, "Granularity:", granularity, "Bin_num:", bin_num))
  tble <- kable_styling(tbl, latex_options = "stripped", full_width = F, protect_latex = TRUE)
  
  return(tble)
}


wrapper.epoche <- function(parameter, dataset_avg, dataset_max, cpu_required, initial_train_size, update_freq, cond.var, bin_num, adjustment, schedule_policy) {
  
  job_length <- as.numeric(parameter[1])
  prob_cut_off <- as.numeric(parameter[2])
  granularity <- as.numeric(parameter[3])
  bin_num <- as.numeric(parameter[4])
  
  output <- ar_logistic_model(dataset_avg, dataset_max, initial_train_size, prob_cut_off, 1, job_length, cpu_required, cond.var, granularity, bin_num)
  
  print(output)
}


## Read back ground job pool

sample_size <- 100
cpu_usage <- 3
total_trace_length <- 8000
initial_train_size <- 6000
adjustment <- FALSE
cond.var <- "lm"

window_sizes <- c(12)
prob_cut_offs <- c(0.01, 0.5)
granularity <- c(10, 100/64)
num_of_bins <- c(1000, 500)

schedule_policy <- "dynamic"

data_matrix_avg <- matrix(nrow = total_trace_length, ncol = 0)
data_matrix_max <- matrix(nrow = total_trace_length, ncol = 0)
for (job_num in bg_job_pool) {
  bg_job <- read.csv(paste(bg_jobs_path, job_num, ".csv", sep = ""))
  data_matrix_avg <- cbind(data_matrix_avg, bg_job$avg_cpu[1:total_trace_length])
  data_matrix_max <- cbind(data_matrix_max, bg_job$max_cpu[1:total_trace_length])
}
rownames(data_matrix_avg) <- seq(1, nrow(data_matrix_avg) ,1)
rownames(data_matrix_max) <- seq(1, nrow(data_matrix_max) ,1)
colnames(data_matrix_avg) <- bg_job_pool
colnames(data_matrix_max) <- bg_job_pool

cpu_required <- rep(0, ncol(data_matrix_max))
for (j in 1:ncol(data_matrix_max)) {
  cpu_required[j] <- as.numeric(quantile(data_matrix_max[,j], c(0.15, 0.5, 0.85), type = 4)[cpu_usage])
}

parameter.df <- expand.grid(window_sizes, prob_cut_offs, granularity, num_of_bins)
colnames(parameter.df) <- c("job_length", "prob_cut_off", "granularity", "num_of_bins")
parameter.df <- parameter.df %>% 
  arrange(job_length)

slt <- apply(parameter.df, 1, wrapper.epoche, data_matrix_avg, data_matrix_max, (100-cpu_required), initial_train_size, 1, cond.var, 100, adjustment, schedule_policy)
```