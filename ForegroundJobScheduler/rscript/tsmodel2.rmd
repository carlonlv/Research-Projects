---
title: "test"
output: html_document
---

```{r setup, include=FALSE}
vmtable <- read.csv("C://Users//carlo//PycharmProjects//ForegroundJobScheduler//datasets//vmtable.csv")
```

```{r}
library("ggplot2")
library("ggfortify")
library("dplyr")
library("aTSA")
library("forecast")
library("mvtnorm")
```


### Histogram of VMs

```{r}
header_for_vmtable <- c('vmid','subscriptionid','deploymentid','vmcreated', 'vmdeleted', 'maxcpu', 'avgcpu', 'p95maxcpu', 'vmcategory', 'vmcorecount', 'vmmemory')
names(vmtable) <- header_for_vmtable
vmtable$lifetime = round((vmtable$vmdeleted - vmtable$vmcreated) / 3600)
```

```{r}
hist(vmtable$lifetime, labels = TRUE)
hist(vmtable$lifetime, plot = FALSE)
```

## Split the Jobs

```{r}
short_vm <- NULL
long_vm <- NULL
for (i in 1:nrow(vmtable)) {
  if (vmtable[i,]$lifetime > 50) {
    long_vm <- c(long_vm, vmtable[i,]$vmid)
  } else {
    short_vm <- c(short_vm, vmtable[i,]$vmid)
  }
}
print("done")
```


## Impletemt Growing Predictive Model

```{r}
cov_cal <- function(variance, l, k, phi) {
  #### input variance: A vector of length min(l,k), var(an+min(l,k)) to var(an+1)
  #### input l: row number of var-cov matrix, minimal value 1
  #### input k: column number of var-cov matrix, minimal value 1
  #### input phi: coeff of AR1 model
  
  phi_mul <- phi^(seq((l+k-2*min(l,k)), l+k-2, by=2))
  result <- sum(phi_mul*variance)
  return(result)
}

calculate_var_cov_matrix_ar1 <-function(var, l, phi) {
  #### input var: A vector from var(an+l) to var(an+1) of length l
  #### input l: number of prediction
  #### input phi: coeff of AR1 model
  var_cov <- matrix(nrow = l, ncol = l)
  for (i in 1:l) {
    for (j in 1:l) {
      variance <- var[seq(l-min(i,j)+1, l, by=1)]
      var_cov[i, j] <- cov_cal(variance, i, j, phi)
    }
  }
  return(var_cov)
}

growing_predictive_model <- function(dataset, initial_train_size, start_pred_time, level, update_freq=1, predict_size, plot=FALSE, plot_freq=update_freq, model_selection="AR1", forgetting_weights) {
  #### input dataset: A time series model or a dataset
  #### input initial_train_size: The number of observations used to train the model
  #### input update_freq: The number of observations for each update of the model, and do the prediction
  #### input start_pred_time: The time to start outputting predictions, starting from the next observation
  #### input level: The upper bound
  #### input predict_size: After how long from start_pred_time do we stop predicting
  #### input plot: optional TRUE of FALSE
  #### input plot_freq: if plot is TRUE, plot after this many observations
  #### input model_selection: AR1 or use auto.arima
  
  ts_model <- NULL
  probability <- data.frame()
  first_time <- 0
  logliklihood <- NULL
  
  if (model_selection == "AR1") {
    ts_model <- arima(dataset[1:initial_train_size], order=c(1,0,0), include.mean = TRUE)
    logliklihood <- c(logliklihood, ts_model$loglik)
    current_end <- initial_train_size
    current_plot <- initial_train_size
    while (current_end < length(dataset)) {
      ## Plot option
      if (current_end >= current_plot & plot== TRUE) {
        plot(ts_model, main = paste("Time Series Plot Up to", current_plot), xlab = "Timestamp", ylab = "Cpu Occupancy")
        current_plot = current_plot + plot_freq
      }
      ## Prediction
      if (current_end >= start_pred_time) {
        if (first_time == 0) {
          first_time = 1
          # Correct to start_pred_time
          current_end <- start_pred_time
        }
        last_obs <- dataset[current_end]
        phi <- as.numeric(ts_model$coef[1])
        mean <- as.numeric(ts_model$coef[2])
        # Construct mean
        mu <- rep(0, predict_size)
        for (i in 1:predict_size) {
          last_obs <- last_obs*phi + (1 - phi) * mean
          mu[i] <- last_obs
        }
        # Construct Var-cov matrix
        var <- rep(ts_model$sigma2, predict_size)
        var_cov <- calculate_var_cov_matrix_ar1(var, predict_size, phi)
        # caclulate probability
        prob_vector <- rep(NA, predict_size)
        up_bound <- rep(level, predict_size)
        for (i in 1:predict_size) {
          prob_vector[i] <- 1 - pmvnorm(upper = up_bound[1:i], mean = mu[1:i], sigma = var_cov[1:i,1:i])
        }
        probability <- rbind(probability, prob_vector)
      }
      
      ## Update Model
      current_end <- current_end + update_freq
      ts_model <- arima(dataset[1:current_end], order=c(1,0,0), include.mean = TRUE)
      logliklihood <- c(logliklihood, ts_model$loglik)
    }
    
    ts_model <- arima(dataset, order=c(1,0,0), include.mean = TRUE)
    if (plot == TRUE) {
      plot(ts_model, main = paste("Time Series Plot Up to", length(dataset)), xlab = "Timestamp", ylab = "Cpu Occupancy")
    }
    last_obs <- dataset[length(dataset)]
    phi <- as.numeric(ts_model$coef[1])
    mean <- as.numeric(ts_model$coef[2])
    # Construct mean
    mu <- rep(0, predict_size)
    for (i in 1:predict_size) {
      last_obs <- last_obs*phi + (1 - phi) * mean
      mu[i] <- last_obs
    }
    # Construct Var-cov matrix
    var <- rep(ts_model$sigma2, predict_size)
    var_cov <- calculate_var_cov_matrix_ar1(var, predict_size, phi)
    # caclulate probability
    prob_vector <- rep(NA, predict_size)
    up_bound <- rep(level, predict_size)
    for (i in 1:predict_size) {
      prob_vector[i] <- 1 - pmvnorm(upper = up_bound[1:i], mean = mu[1:i], sigma = var_cov[1:i, 1:i])
    }
    probability <- rbind(probability, prob_vector)
    return(probability)
      
  } else if (model_selection == "Auto") {
    
    return(NULL)
  } else {
    print("model_selection should be 'AR1' or 'Auto'.")
  }
}
```

Suppose we use AR(1) model for the entire model, and $\{Z_t\}$ is the process with zero mean, i.e., the original time series subtracted by its mean.

Then $Z_t = \phi Z_{t-1} + a_t$ where $a_t$ is theoretically white noise process with mean 0 and variance 1.

Using MA representation, $Z_t = \sum_{j=0}^\infty \theta_jB^ja_t = \frac{1}{1-\phi B}a_t$, then
\[(\theta_0+\theta_1B + \theta_2B^2 + \theta_3B^3 +...)(1-\phi B) = 1\]
\[(\theta_0 + (\theta_1 - \phi\theta_0)B + (\theta_2 - \phi\theta_1)B^2 +...) = 1\]

This implies that $\theta_j = \phi^j, \enspace \forall j \geq 0$, that is, $Z_t = a_t + \phi a_{t-1} + \phi^2 a_{t-2} + ...$, let $t = n+l$, then $Z_{n+l} = a_{n+l} + \phi a_{n+l-1} + \phi^2a_{n+l-2} + ... + \phi^l a_{n} + \phi^{l+1}a_{n-1}+...$.

Using least squares estimates, the $l$th estimate after $n$ is, $\hat{Z}_n(l) = \theta_la_n + \theta_{l+1}a_{n-1} + ...$. Since $E[a_{n+j}|Z_n, Z_{n-1},...] = 0$ 

if $j>0$, $E[a_{n+j}|Z_n, Z_{n-1},...] = a_{n+j}$ if $j \leq 0$, then
$E[Z_{n+l}|Z_n, Z_{n-1}] = \theta_l a_n + \theta_{l+1}a_{n-1} +...= \hat{Z}_n(l)$.

The forecast error, $e_n(l) = Z_{n+l} - \hat{Z}_n(l) = \sum_{j=0}^{l-1}\theta_ja_{n+l-j}$. 

$E[e_n(l)] = 0$, $Var[e_n(l)] = \sum_{j=0}^{l-1}\theta_j^2 Var(a_{n+l-j})$.

$Cov[e_n(l), e_n(k)] = \sum_{p=0}^{min(l,k)-1}\theta_{l-1-p}\theta_{k-1-p}Var(a_{n+1+p})$.


## Analysis on Sample Job 1

```{r}
vmjob1 <- read.csv("C://Users//carlo//PycharmProjects//ForegroundJobScheduler//datasets//csvalldata//1227464.csv")
```

\textbf{1. Time Series Plot}

```{r}
ts_model1 <- ts(data = vmjob1$max_cpu, start = 0, frequency = 300)
autoplot(ts_model1) + 
  ggtitle("Figure 1. Time Series Plot of Job 1") + 
  ylab("Cpu Occupancy") + 
  xlab("Timestamp") +
  theme(plot.title = element_text(size=8, face="bold"))
```

\textbf{2. Fit AR1 Model}

```{r}
growing_predictive_model(dataset=vmjob1$max_cpu, initial_train_size=200, start_pred_time=250, level=40, update_freq=1, predict_size=5, plot=FALSE, plot_freq=100, model_selection="AR1", forgetting_weights=NULL)
```


## Analysis on Sample Job 2

```{r}
vmjob2 <- read.csv("C://Users//carlo//PycharmProjects//ForegroundJobScheduler//datasets//csvalldata//1926479.csv")
```

\textbf{1. Time Series Plot}

```{r}
ts_model2 <- ts(data = vmjob2$max_cpu, start = 0, frequency = 300)
autoplot(ts_model2) + 
  ggtitle("Figure 2. Time Series Plot of Job 2") + 
  ylab("Cpu Occupancy") + 
  xlab("Timestamp") +
  theme(plot.title = element_text(size=8, face="bold"))
```

\textbf{2. Fit AR1 Model}

```{r}
growing_predictive_model(dataset=vmjob2$max_cpu, initial_train_size=200, start_pred_time=250, level=50, update_freq=5, predict_size=5, plot=FALSE, plot_freq=100, model_selection="AR1", forgetting_weights=NULL)
```